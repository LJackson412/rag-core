from __future__ import annotations

from typing import Annotated, Literal, Type, TypeVar

from langchain_core.runnables import RunnableConfig, ensure_config
from pydantic import BaseModel, Field

from rag_core.retrieval.prompts import (
    COMPRESS_DOCS_PROMPT,
    GENERATE_ANSWER_PROMPT,
    GENERATE_QUESTIONS_PROMPT,
)
from rag_core.retrieval.schema import LLMAnswer

T = TypeVar("T", bound="RetrievalConfig")


class RetrievalConfig(BaseModel):
    """Configurable Indexing Mode for RAG Index Graph."""

    collection_id: Annotated[
        str,
        Field(
            default="",
            description=(
                "By specifying a collection, all documents in the container will be queried"
            ),
            json_schema_extra={
                "langgraph_nodes": ["retrieve"],
            },
        ),
    ]
    doc_id: Annotated[
        str | None,
        Field(
            default=None,
            description="If no document ID is specified, the entire collection is queried.",
            json_schema_extra={
                "langgraph_nodes": ["retrieve"],
            },
        ),
    ]
    generate_questions_model: Annotated[
        Literal["openai/gpt-4.1", "openai/gpt-4.1-mini"],
        Field(
            default="openai/gpt-4.1",
            description=(
                "Model for generating semantic question variants. "
                "Use 'gpt-4.1' for complex queries; 'gpt-4.1-mini' for lower cost."
            ),
            json_schema_extra={
                "langgraph_nodes": ["generate_questions"],
            },
        ),
    ]
    generate_questions_prompt: str = Field(
        default=GENERATE_QUESTIONS_PROMPT,
        description=(
            "The prompt used to generate semantic search query variants."
            "The default prompt is designed for the multi-query technique."
            "If you change the prompt, you can use different techniques:"
            "such as sub-query, query rewriting, HyDE, step-back."
        ),
        json_schema_extra={
            "langgraph_nodes": ["generate_questions"],
            "langgraph_type": "prompt",
        },
    )
    number_of_llm_generated_questions: Annotated[
        int,
        Field(
            default=3,
            description=(
                "The number of search queries generated by an LLM"
                "A database query is performed for each generated search query"
            ),
            json_schema_extra={
                "langgraph_nodes": ["generate_questions"],
            },
        ),
    ]

    embedding_model: Annotated[
        Literal[
            "openai/text-embedding-3-small",
            "openai/text-embedding-3-large",
        ],
        Field(
            default="openai/text-embedding-3-large",
            description=(
                "Embedding model used for indexing "
                "Use 'text-embedding-3-large' when you need maximum retrieval quality"
                "Use the same embedding model that was used to index the collection/document"
            ),
            json_schema_extra={
                "langgraph_nodes": ["retrieve"],
            },
        ),
    ]
    number_of_docs_to_retrieve: Annotated[
        int,
        Field(
            default=10,
            description=(
                "For each search query generated by llm, the configured number of documents is retrieved from the database."
                "Please note that duplicate documents may be returned for the same search query; these will be removed."
            ),
            json_schema_extra={
                "langgraph_nodes": ["retrieve"],
            },
        ),
    ]
    include_original_question: bool = Field(
        default=True,
        description="If false only llm generated questions used for retrieval",
        json_schema_extra={"langgraph_nodes": ["retrieve"]},
    )
    compress_docs_model: Annotated[
        Literal["openai/gpt-4.1", "openai/gpt-4.1-mini"],
        Field(
            default="openai/gpt-4.1",
            description=(
                "2-Stage of Retrieval: Compression Model for classifying document chunk relevance. "
                "Use 'gpt-4.1' for subtle relevance; 'gpt-4.1-mini' for unified mini usage."
            ),
            json_schema_extra={
                "langgraph_nodes": ["compress_docs"],
            },
        ),
    ]
    compress_docs_prompt: str = Field(
        default=COMPRESS_DOCS_PROMPT,
        description="The prompt used to classify document chunk relevance.",
        json_schema_extra={
            "langgraph_nodes": ["compress_docs"],
            "langgraph_type": "prompt",
        },
    )

    generate_answer_model: Annotated[
        Literal["openai/gpt-4.1", "openai/gpt-4.1-mini"],
        Field(
            default="openai/gpt-4.1",
            description=(
                "Model for generating final grounded answers. "
                "Use 'gpt-4o' as an alternative; mini variants for cost/latency optimizations."
            ),
            json_schema_extra={
                "langgraph_nodes": ["generate_answer"],
            },
        ),
    ]
    vstore: Annotated[
        Literal["chroma"],
        Field(
            default="chroma",
            description="Vector store provider",
            json_schema_extra={
                "langgraph_nodes": ["retrieve"],
            },
        ),
    ]
    generate_answer_prompt: str = Field(
        default=GENERATE_ANSWER_PROMPT,
        description="The system prompt used for generating responses.",
        json_schema_extra={
            "langgraph_nodes": ["generate_answer"],
            "langgraph_type": "prompt",
        },
    )
    generate_answer_schema: Annotated[
        Type[BaseModel],
        Field(
            default=LLMAnswer,
            description=(
                "Pydantic schema used to structure the final answer output. "
                "Defaults to LLMAnswer when no custom schema is provided."
            ),
            json_schema_extra={
                "langgraph_nodes": ["generate_answer"],
            },
        ),
    ]

    @classmethod
    def from_runnable_config(cls: type[T], config: RunnableConfig | None = None) -> T:
        """Create an RetrievalConfiguration instance from a RunnableConfig object."""
        config = ensure_config(config)
        configurable = config.get("configurable", {}) or {}
        valid_fields = set(cls.model_fields.keys())
        filtered = {k: v for k, v in configurable.items() if k in valid_fields}
        return cls(**filtered)
